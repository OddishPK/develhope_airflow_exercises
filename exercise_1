from airflow import DAG 
from airflow.operators.bash_operator import BashOperator 
from datetime import datetime, timedelta

default_dag_args = {
    'start_date': datetime(2023, 1, 11),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=3),
    'project_id': 1
}

with DAG("first_dag", schedule_interval = None, default_args = default_dag_args) as dag:
    #define tasks of DAG here
    task_0 = BashOperator(task_id = 'bash_task', bash_command = "echo 'Command executed from Bash Operator via DAG' ")
    task_1 = BashOperator(task_id = 'bash_task_move_data', bash_command = r"cp C:\Documents\Work\Data_Center\Data_Lake\dataset_raw.txt C:\Documents\Work\Data_Center\Clean_Data")
    task_2 = BashOperator(task_id = 'bash_task_move_data', bash_command = r"rm C:\Documents\Work\Data_Center\Data_Lake\dataset_raw.txt")
    
    task_0 >> task_1 >> task_2
